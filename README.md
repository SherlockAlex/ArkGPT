# ArkGPT神经网络模型简介
基于Linear-Attention的无限上下文神经网络模型，仅仅只是一个玩具模型，但对于一个语言模型来说是完整的。

此模型尝试解决传统大语言模型内存开销过高、线性注意力过于弥散的问题，采用以时间换空间的策略进行开发。
# ArkGPT模型架构
![模型架构](./Model%20Structure/ModelStructure.png)

# 注意力机制
注意力机制的定义式如下：
    $$attention = \frac{\sum_{i}^{n} sim(Q_{j}K_{i}^{T})V_{i}}{\sum_{i}^{n}sim(Q_{j}K_{i}^{T})+eps}$$

其中，映射
    $sim(x) \in [0,1] $
一般选取sim(x) = exp(x)，也就是Softmax注意力机制，但是这样会增大内存开销，原因是每个
    $K_{i}^{T}$
都必须同时与同一个矢量
    $Q_{j}$
内积之后才能和
    $V_{i}$
作用，这个步骤的空间复杂度是
    $O(n^2)$
级别的，n为上下文长度，但是如果先处理
    $K_{i}^{T}V_{i}$
后再去作用
    $Q_{j}$
，可以降低空间复杂度为
    $O(n)$

# 线性注意力机制
虽然Softmax注意力机制在训练时可以采用并行计算，训练速度快，但是具有内存资源开销大、推理速度慢等缺点。采用线性注意力可以成功规避掉这些缺点。

* 基本假定:
    $$sim(QK^{T}) = \psi(Q)\psi(K)^{T}$$

* 核函数选取: 
    $$\psi(x) = [elu(x)+1]^{n}$$
    $n\in \{1,2,3……\}$
，当
    $n$
越大，注意力越集中，而当
    $n$
越小，注意力越涣散。但同时
    $n$
越大，越容易出现数值爆炸的问题，项目中$n = 5$

* 线性注意力的公式如下：
    $$a_{j} = \frac{\sum_{i}^{n} [\psi (Q_{j})\psi (K_{i})^{T}V_{i}]}{\sum_{i}^{n}\psi (Q_{j})\psi (K_{i})^{T}+eps }$$

* Memory项为：
    $$M(n) = \beta\cdot M(n-1) + \psi (K_{n})^{T}V_{n}$$

* 归一化Zeta项为：
    $$Z(n) = \beta\cdot Z(n-1) + \psi (K_{n})^{T}$$

* 线性注意力表现为:
    $$a_{j} = \frac{\psi (Q_{j})M(n)}{\psi(Q_{j})Z(n) + eps}$$

其中，eps为无穷小量，目的是为了训练的稳定性，防止出现分母为0的情况。

$\beta$
表示模型的遗忘系数，防止应为上下文长度过长而发生数值爆炸的情况，为了保证上下文长度。

模型的遗忘系数为
$\beta = 1-10^{-7}$
，可参考的稳定在上下文长度为200w到400w之间，
$\beta$
越接近1，可参考上下文长度越长，且不增加内存资源的开销
。

# 优点
* 解决线性注意力弥散、不够集中的问题。
* 可以实现对话上下文长度增加，不增加内存开销。
# 缺点
* 增加核函数的幂次方时可以改进线性注意力的弥散问题，却也带来了数值爆炸的风险。
* 模型无法并行计算，训练时间被拉长。
# 项目文件
    Model.py:保存着神经网络层。
    ArkGPT.py:模型封装。
    run.py:模型运行器。
    train.py:模型训练器。
    traindata.py:数据集处理文件，若想训练模型，请编写文件中的两个函数。

